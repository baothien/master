@inproceedings{csurka2004visual,
    abstract = {{Abstract. We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Na\~{A}¯ve Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying seven semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information. 1.}},
    author = {Csurka, Gabriella and Dance, Christopher R. and Fan, Lixin and Willamowski, Jutta and Bray, C\~{A}{\copyright}dric},
    booktitle = {In Workshop on Statistical Learning in Computer Vision, ECCV},
    citeulike-article-id = {4007958},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.604},
    keywords = {acmsiggrahp2011, histogram, recognition},
    pages = {1--22},
    posted-at = {2013-01-29 18:42:18},
    priority = {2},
    title = {{Visual categorization with bags of keypoints}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.604},
    year = {2004}
}

@inproceedings{uijlings2009what,
    abstract = {{This paper discusses the question: Can we improve the recognition of objects by using their spatial context? We start from Bag-of-Words models and use the Pascal 2007 dataset. We use the rough object bounding boxes that come with this dataset to investigate the fundamental gain context can bring. Our main contributions are: (I) The result of Zhang et al. in CVPR07 that context is superfluous derived from the Pascal 2005 data set of 4 classes does not generalize to this dataset. For our larger and more realistic dataset context is important indeed. (II) Using the rough bounding box to limit or extend the scope of an object during both training and testing, we find that the spatial extent of an object is determined by its category: (a) well-defined, rigid objects have the object itself as the preferred spatial extent. (b) Non-rigid objects have an unbounded spatial extent : all spatial extents produce equally good results. (c) Objects primarily categorised based on their function have the whole image as their spatial extent. Finally, (III) using the rough bounding box to treat object and context separately, we find that the upper bound of improvement is 26\% (12\% absolute) in terms of mean average precision, and this bound is likely to be higher if the localisation is done using segmentation. It is concluded that object localisation, if done sufficiently precise, helps considerably in the recognition of objects for the Pascal 2007 dataset.}},
    author = {Uijlings, J. R. R. and Smeulders, A. W. M. and Scha, R. J. H.},
    booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
    citeulike-article-id = {11969780},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/CVPR.2009.5206663},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5206663},
    doi = {10.1109/CVPR.2009.5206663},
    institution = {Intell. Syst. Lab., Univ. of Amsterdam, Amsterdam, Netherlands},
    isbn = {978-1-4244-3992-8},
    issn = {1063-6919},
    keywords = {acmsiggrahp2011},
    month = jun,
    pages = {770--777},
    posted-at = {2013-01-29 18:14:11},
    priority = {2},
    publisher = {IEEE},
    title = {{What is the spatial extent of an object?}},
    url = {http://dx.doi.org/10.1109/CVPR.2009.5206663},
    year = {2009}
}

@article{zhang2007local,
    abstract = {{Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover's Distance and the \^{A}¿2 distance. We first evaluate the performance of our approach with different keypoint detectors and descriptors, as well as different kernels and classifiers. We then conduct a comparative evaluation with several state-of-the-art recognition methods on four texture and five object databases. On most of these databases, our implementation exceeds the best reported results and achieves comparable performance on the rest. Finally, we investigate the influence of background correlations on recognition performance via extensive tests on the PASCAL database, for which ground-truth object localization information is available. Our experiments demonstrate that image representations based on distributions of local features are surprisingly effective for classification of texture and object images under challenging real-world conditions, including significant intra-class variations and substantial background clutter.}},
    address = {Hingham, MA, USA},
    author = {Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.},
    citeulike-article-id = {1127016},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1227526.1227537},
    citeulike-linkout-1 = {http://dblp.uni-trier.de/rec/bibtex/journals/ijcv/ZhangMLS07},
    citeulike-linkout-2 = {http://dx.doi.org/10.1007/s11263-006-9794-4},
    citeulike-linkout-3 = {http://www.ingentaconnect.com/content/klu/visi/2007/00000073/00000002/00009794},
    doi = {10.1007/s11263-006-9794-4},
    issn = {0920-5691},
    journal = {Int. J. Comput. Vision},
    keywords = {acmsiggrahp2011},
    month = jun,
    number = {2},
    pages = {213--238},
    posted-at = {2013-01-29 18:11:16},
    priority = {2},
    publisher = {Kluwer Academic Publishers},
    title = {{Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study}},
    url = {http://dx.doi.org/10.1007/s11263-006-9794-4},
    volume = {73},
    year = {2007}
}

@inproceedings{zhu2004car,
    abstract = {{In this paper we present a novel fast multi-cues based car detection technique in still outdoor images. On the bottom level, two novel area templates based on edge cue and interest points cue are first designed, which can rapidly reject most of the non-car sub-windows at the cost of missing few of the car sub-windows. On the top level, both global structure cue and local texture cue are considered. To character the global structure property the odd Gabor moments are introduced and trained by SVMs. The multi channels even Gabor based local texture property extracted from corner area is modeled as a Gaussian distribution. The final experiment results show that the integration of global structure property and local texture property is more powerful in discrimination between car and non-car objects and a high detection accurate 93\% is obtained.}},
    address = {Washington, DC, USA},
    author = {Zhu, Zhenfeng and Lu, Hanqing and Hu, James and Uchimura, Keiichi},
    booktitle = {Proceedings of the Pattern Recognition, 17th International Conference on (ICPR'04) Volume 2 - Volume 02},
    citeulike-article-id = {11962632},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1020807},
    citeulike-linkout-1 = {http://dx.doi.org/10.1109/ICPR.2004.229},
    doi = {10.1109/ICPR.2004.229},
    isbn = {0-7695-2128-2},
    keywords = {acmsiggrahp2011},
    pages = {699--702},
    posted-at = {2013-01-27 20:04:50},
    priority = {2},
    publisher = {IEEE Computer Society},
    series = {ICPR '04},
    title = {{Car Detection Based on Multi-Cues Integration}},
    url = {http://dx.doi.org/10.1109/ICPR.2004.229},
    year = {2004}
}

@article{bay2008surf,
    abstract = {{This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision.}},
    address = {New York, NY, USA},
    author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Van Gool, Luc},
    citeulike-article-id = {4544518},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1370556},
    citeulike-linkout-1 = {http://dx.doi.org/10.1016/j.cviu.2007.09.014},
    doi = {10.1016/j.cviu.2007.09.014},
    issn = {1077-3142},
    journal = {Comput. Vis. Image Underst.},
    keywords = {acmsiggrahp2011},
    month = jun,
    number = {3},
    pages = {346--359},
    posted-at = {2013-01-27 20:00:05},
    priority = {2},
    publisher = {Elsevier Science Inc.},
    title = {{Speeded-Up Robust Features (SURF)}},
    url = {http://dx.doi.org/10.1016/j.cviu.2007.09.014},
    volume = {110},
    year = {2008}
}

@inproceedings{dalal2005histograms,
    abstract = {{We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.}},
    address = {Washington, DC, USA},
    author = {Dalal, N. and Triggs, B.},
    booktitle = {Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on},
    citeulike-article-id = {3047126},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1069007},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/CVPR.2005.177},
    citeulike-linkout-2 = {http://dx.doi.org/10.1109/CVPR.2005.177},
    citeulike-linkout-3 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1467360},
    doi = {10.1109/CVPR.2005.177},
    institution = {INRIA Rhone-Alps, Montbonnot, France},
    isbn = {0-7695-2372-2},
    issn = {1063-6919},
    journal = {Computer Vision and Pattern Recognition, IEEE Computer Society Conference on},
    keywords = {acmsiggrahp2011},
    location = {San Diego, CA, USA},
    month = jun,
    pages = {886--893 vol. 1},
    posted-at = {2013-01-27 19:56:41},
    priority = {2},
    publisher = {IEEE},
    series = {CVPR '05},
    title = {{Histograms of oriented gradients for human detection}},
    url = {http://dx.doi.org/10.1109/CVPR.2005.177},
    volume = {1},
    year = {2005}
}

@inproceedings{monzo2008hog,
    abstract = {{This paper presents a comparison between a new face recognition algorithm based on EBGM which replaces Gabor features by HOG descriptors and the original EBGM. The experiments results show a better performance behavior using public available databases. This better performance is explained by the properties of HOG descriptors which are more robust to changes in illumination, rotation and small displacements, and to the higher accuracy of the face graphs obtained compared to classical Gabor-EBGM ones.}},
    author = {Monzo, D. and Albiol, A. and Sastre, J.},
    booktitle = {Image Processing, 2008. ICIP 2008. 15th IEEE International Conference on},
    citeulike-article-id = {11962625},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ICIP.2008.4712085},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4712085},
    doi = {10.1109/ICIP.2008.4712085},
    institution = {I-TEAM, Univ. Politec. de Valencia, Valencia},
    isbn = {978-1-4244-1765-0},
    issn = {1522-4880},
    keywords = {acmsiggrahp2011},
    month = oct,
    pages = {1636--1639},
    posted-at = {2013-01-27 19:50:35},
    priority = {2},
    publisher = {IEEE},
    title = {{HOG-EBGM vs. Gabor-EBGM}},
    url = {http://dx.doi.org/10.1109/ICIP.2008.4712085},
    year = {2008}
}

@article{lowe2004distintive,
    abstract = {{This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.}},
    address = {Hingham, MA, USA},
    author = {Lowe, David G.},
    booktitle = {International Journal of Computer Vision},
    citeulike-article-id = {678563},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=996342},
    citeulike-linkout-1 = {http://dblp.uni-trier.de/rec/bibtex/journals/ijcv/Lowe04},
    citeulike-linkout-2 = {http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94},
    citeulike-linkout-3 = {http://www.springerlink.com/content/h4l02691327px768},
    day = {1},
    doi = {10.1023/B:VISI.0000029664.99615.94},
    issn = {0920-5691},
    journal = {Int. J. Comput. Vision},
    keywords = {acmsiggrahp2011},
    month = nov,
    number = {2},
    pages = {91--110},
    posted-at = {2013-01-27 19:48:22},
    priority = {2},
    publisher = {Kluwer Academic Publishers},
    title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
    url = {http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94},
    volume = {60},
    year = {2004}
}

@inproceedings{coleman2007integrated,
    abstract = {{Corner detection is used in many computer vision applications that require fast and efficient feature matching. For tasks such as robot localisation and navigation, the use of corners for matching is preferred over edges or other, larger, features. In recent years finite-element based methods have been used to develop gradient operators for edge detection that have improved angular accuracy over standard techniques. We extend this work to corner detection, enabling edge and corner detection to be integrated. We demonstrate that accuracy is comparable to well-known existing corner detectors, and that significantly reduced computation time can be achieved, making the approach appropriate for real-time computer vision and robotics.}},
    author = {Coleman, S. and Scotney, B. and Kerr, D.},
    booktitle = {Image Analysis and Processing, 2007. ICIAP 2007. 14th International Conference on},
    citeulike-article-id = {11962622},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ICIAP.2007.4362851},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4362851},
    doi = {10.1109/ICIAP.2007.4362851},
    institution = {Univ. of Ulster, Coleraine},
    isbn = {978-0-7695-2877-9},
    keywords = {acmsiggrahp2011},
    pages = {653--658},
    posted-at = {2013-01-27 19:41:07},
    priority = {2},
    publisher = {IEEE},
    title = {{Integrated Edge and Corner Detection}},
    url = {http://dx.doi.org/10.1109/ICIAP.2007.4362851},
    year = {2007}
}

@inproceedings{ferrari2007learning,
    author = {Ferrari, V. and Zisserman, A.},
    booktitle = {Advances in Neural Information Processing Systems},
    citeulike-article-id = {11962605},
    keywords = {acmsiggrahp2011},
    location = {Vancouver, CA},
    month = dec,
    posted-at = {2013-01-27 18:58:16},
    priority = {2},
    title = {{Learning Visual Attributes}},
    year = {2007}
}

@inproceedings{larmpert2009learning,
    abstract = {{We study the problem of object classification when training and test classes are disjoint, i.e. no training examples of the target classes are available. This setup has hardly been studied in computer vision research, but it is the rule rather than the exception, because the world contains tens of thousands of different object classes and for only a very few of them image, collections have been formed and annotated with suitable class labels. In this paper, we tackle the problem by introducing attribute-based classification. It performs object detection based on a human-specified high-level description of the target objects instead of training images. The description consists of arbitrary semantic attributes, like shape, color or even geographic information. Because such properties transcend the specific learning task at hand, they can be pre-learned, e.g. from image datasets unrelated to the current task. Afterwards, new classes can be detected based on their attribute representation, without the need for a new training phase. In order to evaluate our method and to facilitate research in this area, we have assembled a new large-scale dataset, ldquoAnimals with Attributesrdquo, of over 30,000 animal images that match the 50 classes in Osherson's classic table of how strongly humans associate 85 semantic attributes with animal classes. Our experiments show that by using an attribute layer it is indeed possible to build a learning object detection system that does not require any training images of the target classes.}},
    author = {Lampert, C. H. and Nickisch, H. and Harmeling, S.},
    booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
    citeulike-article-id = {11962563},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/CVPR.2009.5206594},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5206594},
    doi = {10.1109/CVPR.2009.5206594},
    institution = {Max Planck Inst. for Biol. Cybern., Tubingen, Germany},
    isbn = {978-1-4244-3992-8},
    issn = {1063-6919},
    keywords = {acmsiggrahp2011},
    month = jun,
    pages = {951--958},
    posted-at = {2013-01-27 18:47:29},
    priority = {2},
    publisher = {IEEE},
    title = {{Learning to detect unseen object classes by between-class attribute transfer}},
    url = {http://dx.doi.org/10.1109/CVPR.2009.5206594},
    year = {2009}
}

@inproceedings{farhadi2009describing,
    abstract = {{We propose to shift the goal of recognition from naming to describing. Doing so allows us not only to name familiar objects, but also: to report unusual aspects of a familiar object (ldquospotty dogrdquo, not just ldquodogrdquo); to say something about unfamiliar objects (ldquohairy and four-leggedrdquo, not just ldquounknownrdquo); and to learn how to recognize new objects with few or no visual examples. Rather than focusing on identity assignment, we make inferring attributes the core problem of recognition. These attributes can be semantic (ldquospottyrdquo) or discriminative (ldquodogs have it but sheep do notrdquo). Learning attributes presents a major new challenge: generalization across object categories, not just across instances within a category. In this paper, we also introduce a novel feature selection method for learning attributes that generalize well across categories. We support our claims by thorough evaluation that provides insights into the limitations of the standard recognition paradigm of naming and demonstrates the new abilities provided by our attribute-based framework.}},
    author = {Farhadi, A. and Endres, I. and Hoiem, D. and Forsyth, D.},
    booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
    citeulike-article-id = {10677533},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/CVPR.2009.5206772},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5206772},
    doi = {10.1109/CVPR.2009.5206772},
    institution = {Comput. Sci. Dept., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA},
    isbn = {978-1-4244-3992-8},
    issn = {1063-6919},
    keywords = {acmsiggrahp2011},
    location = {Miami, FL},
    month = jun,
    pages = {1778--1785},
    posted-at = {2013-01-27 18:44:01},
    priority = {2},
    publisher = {IEEE},
    title = {{Describing objects by their attributes}},
    url = {http://dx.doi.org/10.1109/CVPR.2009.5206772},
    year = {2009}
}

@inproceedings{vogel2004natural,
    abstract = {{Abstract. In this paper, we present an approach for the retrieval of natural scenes based on a semantic modeling step. Semantic modeling stands for the classification of local image regions into semantic classes such as grass, rocks or foliage and the subsequent summary of this information in so-called conceptoccurrence vectors. Using this semantic representation, images from the scene categories coasts,rivers/lakes,forests,plains,mountains and sky/clouds are retrieved. We compare two implementations of the method quantitatively on a visually diverse database of natural scenes. In addition, the semantic modeling approach is compared to retrieval based on low-level features computed directly on the image. The experiments show that semantic modeling leads in fact to better retrieval performance. 1}},
    author = {Vogel, Julia and Schiele, Bernt},
    booktitle = {In CIVR},
    citeulike-article-id = {11962560},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.3559},
    keywords = {acmsiggrahp2011},
    posted-at = {2013-01-27 18:31:37},
    priority = {2},
    title = {{Natural Scene Retrieval Based on a Semantic Modeling Step}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.3559},
    year = {2004}
}

@article{van2009learning,
    abstract = {{Color names are required in real-world applications such as image retrieval and image annotation. Traditionally, they are learned from a collection of labeled color chips. These color chips are labeled with color names within a well-defined experimental setup by human test subjects. However, naming colors in real-world images differs significantly from this experimental setting. In this paper, we investigate how color names learned from color chips compare to color names learned from real-world images. To avoid hand labeling real-world images with color names, we use Google image to collect a data set. Due to the limitations of Google image, this data set contains a substantial quantity of wrongly labeled data. We propose several variants of the PLSA model to learn color names from this noisy data. Experimental results show that color names learned from real-world images significantly outperform color names learned from labeled color chips for both image retrieval and image annotation.}},
    author = {van de Weijer, J. and Schmid, C. and Verbeek, J. and Larlus, D.},
    citeulike-article-id = {7011057},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/TIP.2009.2019809},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4982667},
    doi = {10.1109/TIP.2009.2019809},
    institution = {Comput. Vision Center, Barcelona},
    issn = {1057-7149},
    journal = {Image Processing, IEEE Transactions on},
    keywords = {acmsiggrahp2011},
    month = jul,
    number = {7},
    pages = {1512--1523},
    posted-at = {2013-01-27 18:25:52},
    priority = {2},
    publisher = {IEEE},
    title = {{Learning Color Names for Real-World Applications}},
    url = {http://dx.doi.org/10.1109/TIP.2009.2019809},
    volume = {18},
    year = {2009}
}

@inproceedings{marcin2007learning,
    author = {Marsza{\l}ek, Marcin and Schmid, Cordelia and Harzallah, Hedi and van de Weijer, Joost},
    citeulike-article-id = {11962554},
    citeulike-linkout-0 = {http://lear.inrialpes.fr/pubs/2007/MSHV07},
    keywords = {acmsiggrahp2011, lear},
    month = oct,
    note = {Visual Recognition Challange workshop, in conjunction with ICCV},
    posted-at = {2013-01-27 18:18:05},
    priority = {2},
    title = {{Learning Object Representations for Visual Object Class Recognition}},
    url = {http://lear.inrialpes.fr/pubs/2007/MSHV07},
    year = {2007}
}

@incollection{grubinger2008advances,
    abstract = {{The general photographic ad-hoc retrieval task of the ImageCLEF 2007 evaluation campaign is described. This task provides both the resources and the framework necessary to perform comparative laboratory-style evaluation of visual information retrieval from generic photographic collections. In 2007, the evaluation objective concentrated on retrieval of lightly annotated images, a new challenge that attracted a large number of submissions: a total of 20 participating groups submitted 616 system runs. This paper summarises the components used in the benchmark, including the document collection and the search tasks, and presents an analysis of the submissions and the results.}},
    address = {Berlin, Heidelberg},
    author = {Grubinger, Michael and Clough, Paul and Hanbury, Allan and M\"{u}ller, Henning},
    chapter = {Overview of the ImageCLEFphoto 2007 Photographic Retrieval Task},
    citeulike-article-id = {11962350},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1428919},
    citeulike-linkout-1 = {http://dx.doi.org/10.1007/978-3-540-85760-0\_57},
    doi = {10.1007/978-3-540-85760-0\_57},
    editor = {Peters, Carol and Jijkoun, Valentin and Mandl, Thomas and M\"{u}ller, Henning and Oard, Douglas W. and {N}as, Anselmo P. and Petras, Vivien and Santos, Diana},
    isbn = {978-3-540-85759-4},
    keywords = {acmsiggrahp2011},
    pages = {433--444},
    posted-at = {2013-01-27 16:30:19},
    priority = {2},
    publisher = {Springer-Verlag},
    title = {{Advances in Multilingual and Multimodal Information Retrieval}},
    url = {http://dx.doi.org/10.1007/978-3-540-85760-0\_57},
    year = {2008}
}

@inproceedings{jing2007canoical,
    abstract = {{The vast majority of the features used in today's commercially deployed image search systems employ techniques that are largely indistinguishable from text-document search - the images returned in response to a query are based on the text of the web pages from which they are linked. Unfortunately, depending on the query type, the quality of this approach can be inconsistent. Several recent studies have demonstrated the effectiveness of using image features to refine search results. However, it is not clear whether (or how much) image-based approach can generalize to larger samples of web queries. Also, the previously used global features often only capture a small part of the image information, which in many cases does not correspond to the distinctive characteristics of the category. This paper explores the use of local features in the concrete task of finding the single canonical images for a collection of commonly searched-for products. Through large-scale user testing, the canonical images found by using only local image features significantly outperformed the top results from Yahoo, Microsoft and Google, highlighting the importance of having these image features as an integral part of future image search engines.}},
    address = {New York, NY, USA},
    author = {Jing, Yushi and Baluja, Shumeet and Rowley, Henry},
    booktitle = {Proceedings of the 6th ACM international conference on Image and video retrieval},
    citeulike-article-id = {2730935},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1282280.1282324},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1282280.1282324},
    doi = {10.1145/1282280.1282324},
    isbn = {978-1-59593-733-9},
    keywords = {acmsiggrahp2011},
    location = {Amsterdam, The Netherlands},
    pages = {280--287},
    posted-at = {2013-01-27 16:22:04},
    priority = {2},
    publisher = {ACM},
    series = {CIVR '07},
    title = {{Canonical image selection from the web}},
    url = {http://dx.doi.org/10.1145/1282280.1282324},
    year = {2007}
}

@article{smeulders2000content,
    abstract = {{Presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap}},
    address = {Washington, DC, USA},
    author = {Smeulders, A. W. M. and Worring, M. and Santini, S. and Gupta, A. and Jain, R.},
    citeulike-article-id = {942093},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=357873},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/34.895972},
    citeulike-linkout-2 = {http://dx.doi.org/10.1109/34.895972},
    citeulike-linkout-3 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=895972},
    day = {06},
    doi = {10.1109/34.895972},
    institution = {Amsterdam Univ.},
    issn = {0162-8828},
    journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
    keywords = {acmsiggrahp2011},
    month = dec,
    number = {12},
    pages = {1349--1380},
    posted-at = {2013-01-27 15:20:43},
    priority = {2},
    publisher = {IEEE},
    title = {{Content-based image retrieval at the end of the early years}},
    url = {http://dx.doi.org/10.1109/34.895972},
    volume = {22},
    year = {2000}
}

@inproceedings{lazebnik2006beyond,
    abstract = {{This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting "spatial pyramid" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralbas "gist" and Lowes SIFT descriptors.}},
    address = {Los Alamitos, CA, USA},
    author = {Lazebnik, S. and Schmid, C. and Ponce, J.},
    booktitle = {Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on},
    citeulike-article-id = {1720832},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1153549},
    citeulike-linkout-1 = {http://doi.ieeecomputersociety.org/10.1109/CVPR.2006.68},
    citeulike-linkout-2 = {http://dblp.uni-trier.de/rec/bibtex/conf/cvpr/LazebnikSP06},
    citeulike-linkout-3 = {http://dx.doi.org/10.1109/CVPR.2006.68},
    citeulike-linkout-4 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1641019},
    day = {09},
    doi = {10.1109/CVPR.2006.68},
    institution = {University of Illinois},
    isbn = {0-7695-2597-0},
    issn = {1063-6919},
    journal = {Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on},
    keywords = {acmsiggrahp2011, als\_nivedita, qualifying\_exam},
    location = {New York, NY, USA},
    month = oct,
    pages = {2169--2178},
    posted-at = {2012-07-27 15:03:23},
    priority = {2},
    publisher = {IEEE},
    title = {{Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories}},
    url = {http://dx.doi.org/10.1109/CVPR.2006.68},
    volume = {2},
    year = {2006}
}

